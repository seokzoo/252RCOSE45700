import sys
import os
import gradio as gr
from functools import lru_cache
from dotenv import load_dotenv

from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import RecursiveCharacterTextSplitter

# .env íŒŒì¼ì—ì„œ API í‚¤ ë¡œë“œ
load_dotenv()
if not os.getenv("OPENAI_API_KEY"):
    print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    sys.exit(1)


@lru_cache(maxsize=1)
def get_llm():
    """LLM ì´ˆê¸°í™” - ì•± ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ ìƒì„±"""
    print("ğŸ¤– LLMì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤... (OpenAI: gpt-4o-mini)")
    return ChatOpenAI(
        model="gpt-4o-mini",
        max_tokens=2000,
        temperature=0.7,
        streaming=True,
    )


@lru_cache(maxsize=4) # PDF ê²½ë¡œë³„ë¡œ ìºì‹œ
def get_retriever_from_pdf(pdf_path: str):
    """PDF íŒŒì¼ë¡œë¶€í„° Knowledge Base Retriever ì´ˆê¸°í™”"""
    if not pdf_path or not os.path.exists(pdf_path):
        print(f"âŒ PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
        # Gradio ì¸í„°í˜ì´ìŠ¤ì—ì„œëŠ” ì˜¤ë¥˜ë¥¼ raiseí•˜ëŠ” ê²ƒì´ ì¢‹ìŒ
        raise gr.Error(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")
    
    try:
        print(f"ğŸ“š PDF ë¬¸ì„œë¥¼ ë¡œë“œí•©ë‹ˆë‹¤: {pdf_path}")
        # 1. ë¬¸ì„œ ë¡œë“œ
        loader = PyPDFLoader(pdf_path)
        docs = loader.load()
        if not docs:
            raise gr.Error(f"PDFì—ì„œ ë¬¸ì„œë¥¼ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {pdf_path}")

        # 2. ë¬¸ì„œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        splits = text_splitter.split_documents(docs)

        # 3. ì„ë² ë”© ë° ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        print("ğŸ§  ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤... (FAISS)")
        embeddings = OpenAIEmbeddings()
        vector_store = FAISS.from_documents(splits, embeddings)

        # 4. Retriever ë°˜í™˜
        return vector_store.as_retriever(
            search_kwargs={"k": 5}
        )
    except Exception as e:
        print(f"âŒ Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        raise gr.Error(f"Retriever ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")


def format_docs(docs):
    """ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (LangChain Document ê°ì²´ìš©)"""
    if not docs:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ")
        return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    formatted = [
        doc.page_content for doc in docs if hasattr(doc, "page_content") and doc.page_content
    ]

    result = (
        "\n\n---\n\n".join(formatted)
        if formatted
        else "ë¬¸ì„œ ë‚´ìš©ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
    )
    print(f"âœ… {len(formatted)}ê°œ ë¬¸ì„œ í¬ë§· ì™„ë£Œ (ì´ {len(result)}ì)")
    return result


def create_chain_with_kb(retriever, llm):
    """RAG ì²´ì¸ ìƒì„± - Retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰ í›„ LLMì— ì „ë‹¬"""
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                """ë‹¤ìŒ ë¬¸ë§¥(context)ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë¬¸ë§¥ì— ë‹µì´ ì—†ìœ¼ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•˜ì„¸ìš”.

Context:
{context}
""",
            ),
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )

    def retrieve_and_format(x):
        """ê²€ìƒ‰ ì‹¤í–‰ ë° í¬ë§·íŒ…"""
        try:
            input_text = x["input"] if isinstance(x, dict) else x
            print(f"\nğŸ” ê²€ìƒ‰ ì¿¼ë¦¬: '{input_text}'")
            retrieved_docs = retriever.invoke(input_text)
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(retrieved_docs) if retrieved_docs else 0}ê°œ")
            return format_docs(retrieved_docs)
        except Exception as e:
            print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"

    return (
        {
            "context": retrieve_and_format,
            "chat_history": lambda x: x["chat_history"],
            "input": lambda x: x["input"],
        }
        | prompt
        | llm
    )


def create_chain_without_kb(llm):
    """ì¼ë°˜ ëŒ€í™”ìš© ì²´ì¸ - KB ì—†ì´ LLMë§Œ ì‚¬ìš©"""
    prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "{input}"),
        ]
    )
    return prompt | llm


# --- Gradio UI ë° ì±„íŒ… ë¡œì§ ---

llm = get_llm() # LLMì€ ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ

def chat_response(user_input, chat_history_list, use_kb, pdf_path):
    """
    Gradioì˜ Chatbot UIì™€ ì—°ë™ë˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    user_input: ì‚¬ìš©ìì˜ ìƒˆ ì…ë ¥ (str)
    chat_history_list: Gradio ì±—ë´‡ì˜ ëŒ€í™” ê¸°ë¡ (List[List[str, str]])
    use_kb: KB ì‚¬ìš© ì—¬ë¶€ (bool)
    pdf_path: PDF íŒŒì¼ ê²½ë¡œ (str)
    """
    
    # 1. Gradioì˜ ëŒ€í™” ê¸°ë¡ì„ LangChain í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    langchain_history = []
    for user_msg, ai_msg in chat_history_list:
        if user_msg:
            langchain_history.append(HumanMessage(content=user_msg))
        if ai_msg:
            langchain_history.append(AIMessage(content=ai_msg))
    
    # 2. ì²´ì¸ ì„ íƒ
    try:
        if use_kb:
            if not pdf_path:
                raise gr.Error("Knowledge Baseë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ PDF íŒŒì¼ ê²½ë¡œë¥¼ ì…ë ¥í•´ì•¼ í•©ë‹ˆë‹¤.")
            # ìºì‹œëœ Retriever ê°€ì ¸ì˜¤ê¸°
            retriever = get_retriever_from_pdf(pdf_path)
            chain = create_chain_with_kb(retriever, llm)
            print("â„¹ï¸ RAG ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.")
        else:
            chain = create_chain_without_kb(llm)
            print("â„¹ï¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œë¡œ ì‘ë‹µí•©ë‹ˆë‹¤.")
            
    except Exception as e:
        # get_retriever_from_pdfì—ì„œ ì˜¤ë¥˜ ë°œìƒ ì‹œ
        yield chat_history_list + [[user_input, str(e)]]
        return

    # 3. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    full_response = ""
    # UIì— ì‚¬ìš©ì ë©”ì‹œì§€ ë¨¼ì € í‘œì‹œ
    chat_history_list.append([user_input, ""])
    
    try:
        # streaming
        for chunk in chain.stream(
            {
                "chat_history": langchain_history, # í˜„ì¬ ì…ë ¥ì„ ì œì™¸í•œ ì´ì „ ê¸°ë¡
                "input": user_input,
            }
        ):
            full_response += chunk.content
            # ë§ˆì§€ë§‰ ë©”ì‹œì§€(AI ì‘ë‹µ)ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            chat_history_list[-1][1] = full_response
            yield chat_history_list
            
    except Exception as e:
        error_msg = f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        print(f"âŒ {error_msg}")
        chat_history_list[-1][1] = error_msg
        yield chat_history_list


def clear_chat():
    """ì±„íŒ… ê¸°ë¡ì„ ì§€ìš°ëŠ” í•¨ìˆ˜"""
    return [], []

def update_status_text(use_kb, pdf_path):
    """KB ìƒíƒœ í‘œì‹œì¤„ ì—…ë°ì´íŠ¸"""
    if use_kb:
        if pdf_path and os.path.exists(pdf_path):
            return f"âœ… KB ì‚¬ìš© ì¤‘ ({os.path.basename(pdf_path)})"
        elif pdf_path:
            return f"âš ï¸ KB ì‚¬ìš© ì²´í¬ë¨ (íŒŒì¼ ê²½ë¡œ í™•ì¸ í•„ìš”)"
        else:
            return f"âš ï¸ KB ì‚¬ìš© ì²´í¬ë¨ (PDF ê²½ë¡œ ì—†ìŒ)"
    else:
        return "â„¹ï¸ ì¼ë°˜ ëŒ€í™” ëª¨ë“œ"

# --- Gradio UI ë ˆì´ì•„ì›ƒ ---

with gr.Blocks(theme=gr.themes.Soft()) as demo:
    gr.Markdown("# ğŸ¤– Chatbot with PDF Knowledge Base (Gradio)")

    with gr.Row():
        # --- 1. ì‚¬ì´ë“œë°” (Streamlitì˜ sidebarì™€ ìœ ì‚¬) ---
        with gr.Column(scale=1, min_width=300):
            gr.Markdown("## âš™ï¸ ì„¤ì •")
            
            pdf_path_input = gr.Textbox(
                label="PDF íŒŒì¼ ê²½ë¡œ",
                value="nov1025.pdf", # ê¸°ë³¸ê°’ ì„¤ì •
                info="RAGì— ì‚¬ìš©í•  PDF íŒŒì¼ì˜ ê²½ë¡œë¥¼ ì…ë ¥í•˜ì„¸ìš”.",
            )
            use_kb_checkbox = gr.Checkbox(
                label="Knowledge Base ì‚¬ìš©", 
                value=True
            )
            status_display = gr.Markdown(
                update_status_text(True, "nov1025.pdf") # ì´ˆê¸° ìƒíƒœ
            )
            
            # ì„¤ì • ë³€ê²½ ì‹œ ìƒíƒœ í…ìŠ¤íŠ¸ ì—…ë°ì´íŠ¸
            use_kb_checkbox.change(fn=update_status_text, inputs=[use_kb_checkbox, pdf_path_input], outputs=status_display)
            pdf_path_input.change(fn=update_status_text, inputs=[use_kb_checkbox, pdf_path_input], outputs=status_display)

        # --- 2. ë©”ì¸ ì±—ë´‡ ì°½ ---
        with gr.Column(scale=4):
            chatbot_display = gr.Chatbot(
                label="Chat",
                height=600,
                bubble_full_width=False
            )
            user_input_textbox = gr.Textbox(
                placeholder="Message OpenAI...",
                label="User Input",
                scale=7
            )
            clear_btn = gr.Button("ğŸ—‘ï¸ Clear Chat")

    # --- 3. ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ì—°ê²° ---
    
    # ì‚¬ìš©ì ì…ë ¥(submit) ì‹œ
    user_input_textbox.submit(
        fn=chat_response,
        inputs=[user_input_textbox, chatbot_display, use_kb_checkbox, pdf_path_input],
        outputs=[chatbot_display]
    ).then(
        fn=lambda: "", # ì…ë ¥ì°½ ë¹„ìš°ê¸°
        inputs=None,
        outputs=user_input_textbox
    )

    # ì±„íŒ… í´ë¦¬ì–´ ë²„íŠ¼ í´ë¦­ ì‹œ
    clear_btn.click(
        fn=clear_chat,
        inputs=None,
        outputs=[user_input_textbox, chatbot_display]
    )

# --- ì•± ì‹¤í–‰ ---
if __name__ == "__main__":
    print("Gradio ì•±ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. http://127.0.0.1:7860 ì—ì„œ í™•ì¸í•˜ì„¸ìš”.")
    demo.launch()
